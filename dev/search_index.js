var documenterSearchIndex = {"docs":
[{"location":"getting-started/#BoostingDecisionTrees.jl","page":"Getting started","title":"BoostingDecisionTrees.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage)\n\nThis project demonstrates how different tree based models handle multiclass classification using the Iris Dataset. By comparing a single split (stump) to a full tree and an adaptive ensemble (AdaBoost), we can show the \"power\" of each approach.","category":"section"},{"location":"getting-started/#Running-the-package","page":"Getting started","title":"Running the package","text":"To use this package, open a Julia REPL and run:\n\npkg> add https://github.com/MateoSkmn/BoostingDecisionTrees.jl\n\njulia> using BoostingDecisionTrees","category":"section"},{"location":"getting-started/#Examples","page":"Getting started","title":"Examples","text":"","category":"section"},{"location":"getting-started/#Load-dataset","page":"Getting started","title":"Load dataset","text":"For easier use you may download the given dataset under 'src/data/Iris.csv'. The dataset can also be found at https://www.kaggle.com/datasets/uciml/iris.\n\njulia>  X,y = load_data(\"path/to/Iris.csv\")\n\nThis loads the 150 samples in a random order. Attention: this method was created especially for the described Iris.csv and might not work for other datasets. You can always use your own dataset as long as X is a Matrix and y is a Vector.","category":"section"},{"location":"getting-started/#Decision-Stump-[Deprecated!]","page":"Getting started","title":"Decision Stump [Deprecated!]","text":"When you have defined a dataset you can start training your classifiers. The most simple one is the decision stump. You can train a stump with the created dataset and predict labels for your dataset. The following example shows you how to use a simple training and test split\n\njulia> stump = train_stump(X[1:100, :], y[1:100])\n\njulia> prediciton = predict_stump(stump, X[101:150, :])\n\njulia> sum(prediciton .== y[101:150]) / size(y[101:150], 1) # Accuracy of the created model","category":"section"},{"location":"getting-started/#Decision-Tree","page":"Getting started","title":"Decision Tree","text":"julia> tree = train_tree(X[1:100, :], y[1:100]; max_depth=5)\n\njulia> tree2 = train_tree(X[1:100, :], y[1:100]) # This will use the same parameters as the code above\n\njulia> prediciton = predict_tree(tree, X[101:150, :])\n\njulia> sum(prediciton .== y[101:150]) / size(y[101:150], 1) # Accuracy of the created model\n\nTODO","category":"section"},{"location":"getting-started/#AdaBoost","page":"Getting started","title":"AdaBoost","text":"AdaBoost is an ensemble learning classifier using multiple weaker learners. Each new learner focuses on correcting the errors made by its predicessors. You can train a model using your dataset. You may also adjust the maximum number of iterations as well as the maximum 'power' of a weaker learner.\n\njulia> ada = train_adaboost(X[1:100, :], y[1:100]; iterations=50, max_alpha=2.5)\n\njulia> ada2 = train_adaboost(X[1:100, :], y[1:100]) # This will use the same parameters as the code above\n\njulia> prediciton = predict(ada, X[101:150, :])\n\njulia> sum(prediciton .== y[101:150]) / size(y[101:150], 1) # Accuracy of the created model","category":"section"},{"location":"getting-started/#Further","page":"Getting started","title":"Further","text":"TODO: In later development you will be able to switch between the splitting criteria 'gini impurity' and 'information gain'. As for now only 'gini impurity' will be used when creating models.","category":"section"},{"location":"#BoostingDecisionTrees","page":"Documentation","title":"BoostingDecisionTrees","text":"Documentation for BoostingDecisionTrees.\n\n","category":"section"},{"location":"#BoostingDecisionTrees.BoostingDecisionTrees","page":"Documentation","title":"BoostingDecisionTrees.BoostingDecisionTrees","text":"BoostingDecisionTrees\n\nA Julia module for decision tree and boosting algorithms, including decision stumps, Gini impurity, and information gain utilities.\n\nOverview\n\nThis module provides tools for training and evaluating simple decision trees and decision stumps, with support for both Gini impurity and information gain as splitting criteria. It is designed for educational purposes and prototyping machine learning models.\n\nFeatures\n\nSplitting Criteria: Supports both Gini impurity and information gain for feature selection.\nUtilities: Includes helper functions for entropy, Gini impurity, and majority voting.\n\nExports\n\nDecision TreeNode Functions:\ntrain_tree: Train a decision tree on a dataset.\npredict: Make predictions using a trained decision tree.\n\nNotes\n\nIntended as a compact educational toolkit and a foundation for experiments with boosting and small decision-tree learners.\n\n\n\n\n\n","category":"module"},{"location":"#BoostingDecisionTrees.AdaBoost","page":"Documentation","title":"BoostingDecisionTrees.AdaBoost","text":"AdaBoost(learner, alphas)\n\nA stronger ensemble learning classifier consisting of multiple weaker learners. \nEach new learner focuses on correcting the errors made by its predicessors.\n\n# Fields\n- `learner::Vector{DecisionTree}`: A collection of DecisionTree objects. Each tree acts as a weak classifier that makes a prediction based on a single feature threshold.\n- `alphas::Vector{Float64}`: A vector of floating-point weights, corresponding to the voting power of a tree. A higher alpha values means the stump was more accurate during the training phase.\n\n\n\n\n\n","category":"type"},{"location":"#BoostingDecisionTrees.DecisionNode","page":"Documentation","title":"BoostingDecisionTrees.DecisionNode","text":"DecisionNode\n\nA decision node in a decision tree that splits data based on a feature and threshold.\n\nFields\n\nfeature::Int: the feature index used for splitting.\nthreshold::Float64: the threshold value for the split.\nleft::TreeNode: the left subtree (samples where feature ≤ threshold).\nright::TreeNode: the right subtree (samples where feature > threshold).\n\n\n\n\n\n","category":"type"},{"location":"#BoostingDecisionTrees.LeafNode","page":"Documentation","title":"BoostingDecisionTrees.LeafNode","text":"LeafNode\n\nA leaf node holding a predicted class label.\n\n\n\n\n\n","category":"type"},{"location":"#BoostingDecisionTrees.TreeNode","page":"Documentation","title":"BoostingDecisionTrees.TreeNode","text":"TreeNode\n\nAbstract type for decision tree nodes.\n\n\n\n\n\n","category":"type"},{"location":"#BoostingDecisionTrees.best_split-Tuple{AbstractVector{<:Real}, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.best_split","text":"best_split(feature, labels)\n\nFind the best threshold to split a feature vector for minimizing Gini impurity.\n\nArguments\n\nfeature::AbstractVector{<:Real}: A vector of numerical feature values.\nlabels::AbstractVector: a vector of class labels (same length as feature)\n\nReturns\n\nbest_threshold::Union{Float64, Nothing}: The best numerical value to split the feature on.\n\nReturns nothing if no split is possible.\n\nbest_gini::Float64: The weighted Gini impurity after the split.\n\nExamples\n\njulia> feature = [1.0, 2.0, 3.0, 4.0];\n\njulia> labels = [\"A\", \"A\", \"B\", \"B\"];\n\njulia> best_threshold, best_gini = best_split(feature, labels)\n(2.5, 0.0)\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.createWeightedDataset-Tuple{AbstractMatrix, AbstractVector, Vector{Float64}}","page":"Documentation","title":"BoostingDecisionTrees.createWeightedDataset","text":"createWeightedDataset(X, y, weights)\n\nCreate a new dataset by sampling rows from `X` and `y`, guided by a \nprobability distribution defined by `weights` where samples with higher weights are more likely to be selected for the new dataset\n\n# Arguments\n- `X::AbstractMatrix`: rows are samples, columns are features.\n- `y::AbstractVector`: class labels for each sample.\n- `weights::Vector{Float64}`: weight of each sample in the given dataset. The sum of all weights should be 1.\n\n# Returns\n- `X_prime`: A resampled matrix of the same dimensions and type as `X`.\n- `y_prime`: A resampled vector of the same length and type as `y`.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.gini_impurity-Tuple{AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.gini_impurity","text":"gini_impurity(classes)\n\nCompute the Gini impurity of a vector of class labels.\n\nArguments\n\nclasses::AbstractVector: A collection of class labels.\n\nReturns\n\nFloat64: The Gini impurity of the input vector. Returns 0 if the input is empty.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.information_gain-Tuple{AbstractVector{<:Real}, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.information_gain","text":"information_gain(X_column::AbstractVector{<:Real}, y::AbstractVector)\n\nCompute the best information gain obtainable by splitting a numeric feature column using a threshold (x ≤ t vs. x > t).\n\nReturns\n\nbest_threshold::Float64: threshold yielding maximum information gain\nbest_gain::Float64: corresponding information gain\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.load_data_iris-Tuple{String}","page":"Documentation","title":"BoostingDecisionTrees.load_data_iris","text":"load_data_iris(path)\n\nLoad the Iris dataset from a CSV file, shuffle the observations, and split  features from labels.\n\nAI Disclaimer\n\nThis helper method was generated by AI\n\nArguments\n\npath::String: The file path to the CSV file (e.g., \"src/data/Iris.csv\").\n\nReturns\n\nX::Matrix: A matrix of feature values (columns 2 through 5).\ny::Vector: A vector of target labels.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.majority_label-Tuple{Any}","page":"Documentation","title":"BoostingDecisionTrees.majority_label","text":"majority_label(y)\n\nReturn the most common class label in y.\n\nArguments\n\ny::AbstractVector: a collection of class labels.\n\nReturns\n\nThe label that appears most frequently in y. If there is a tie, one of the tied labels is returned.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.predict-Tuple{AdaBoost, AbstractMatrix}","page":"Documentation","title":"BoostingDecisionTrees.predict","text":"predict(model, X)\n\nPredict class labels for samples in X using a trained AdaBoost classifier.\n\nThe function adds the weighted votes of all decision trees within the model to determine the most likely class for each sample.\n\nArguments\n\nmodel::AdaBoost: A trained AdaBoost structure.\nX::AbstractMatrix: rows are samples, columns are features.\n\nReturns\n\nVector: A vector of predicted labels, with the same type as the labels found  in the model's learners.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.predict-Tuple{AdaBoost, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.predict","text":"predict(model::AdaBoost, X::AbstractVector)\n\nA convenience method for predicting the label of a single sample.\n\nArguments\n\nmodel::AdaBoost: A trained AdaBoost structure.\nX::AbstractVector: A single sample represented as a vector of features.\n\nReturns\n\nThe predicted label for the single input sample.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.predict-Tuple{TreeNode, AbstractMatrix}","page":"Documentation","title":"BoostingDecisionTrees.predict","text":"predict(tree::TreeNode, X::AbstractMatrix)\n\nPredict class labels for multiple samples.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.predict-Tuple{TreeNode, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.predict","text":"predict(node::TreeNode, x)\n\nPredict the class label for a single sample.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.train_adaboost-Union{Tuple{T}, Tuple{AbstractMatrix, AbstractVector{T}}} where T","page":"Documentation","title":"BoostingDecisionTrees.train_adaboost","text":"train_adaboost(X, y; iterations, max_alpha)\n\nTrains an AdaBoost classifier on the given dataset.\n\n# Arguments\n- `X::AbstractMatrix`: rows are samples, columns are features.\n- `y::AbstractVector`: class labels for each sample.\n- `iterations::Integer`: maximum number of weak learners. In case of perfect fit the training will be stopped early. Values must be in range [1, Inf). Default is 50.\n- `max_alpha::Float64`: A threshold to cap the \"amount of say\" (alpha) for any single stump. Default is 2.5 which means an accuracy of >= 99.999%. The bigger the value the more of a 'dictator' becomes a stump with a perfect result.\n- `max_depth::Integer`: Maximum depth of each tree. Default is set to 1 which is equivalent to a decision stump\n\n# Returns\n- `AdaBoost`: a trained classifier with `learners` and `alphas`.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.train_tree-Tuple{AbstractMatrix, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.train_tree","text":"train_tree(X, y; max_depth=5, criterion=:gini)\n\nTrain a decision tree using numeric threshold splits.\n\nArguments\n\nX::AbstractMatrix: feature matrix.\ny::AbstractVector: class labels.\nmax_depth::Int: maximum tree depth.\ncriterion::Symbol: :information_gain or :gini.\n\nReturns\n\nTreeNode\n\n\n\n\n\n","category":"method"}]
}
