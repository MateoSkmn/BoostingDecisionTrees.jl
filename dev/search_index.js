var documenterSearchIndex = {"docs":
[{"location":"getting-started/#BoostingDecisionTrees.jl","page":"Getting started","title":"BoostingDecisionTrees.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage)","category":"section"},{"location":"getting-started/#Running-the-package-locally","page":"Getting started","title":"Running the package locally","text":"To use this package locally, open a Julia REPL inside the project structure and run:\n\njulia> ]activate .\n\njulia> include(\"src/BoostingDecisionTrees.jl\")\n\njulia> using .BoostingDecisionTrees","category":"section"},{"location":"getting-started/#Examples","page":"Getting started","title":"Examples","text":"","category":"section"},{"location":"getting-started/#Gini-Impurity","page":"Getting started","title":"Gini Impurity","text":"First define your dataset that you want to train on.\n\njulia>  X = [1 2; 2 3; 11 21; 12 22]\n\njulia> y = [0, 0, 1, 1]\n\njulia> stump = train_stump(X, y)\n\nPlease keep in mind that the current implementation only allows a matrix for X! You can test with just one feature by reshaping a Vector:\n\njulia> x = [1,2,3,4]\n\njulia> X = reshape(x, :, 1)\n\njulia> stump = train_stump(X, y)\n\nNow you can use the created stump to predict the labels for an input matrix, e.g.:\n\njulia>  predict_stump(stump, X)","category":"section"},{"location":"#BoostingDecisionTrees","page":"Documentation","title":"BoostingDecisionTrees","text":"Documentation for BoostingDecisionTrees.\n\n","category":"section"},{"location":"#BoostingDecisionTrees.BoostingDecisionTrees","page":"Documentation","title":"BoostingDecisionTrees.BoostingDecisionTrees","text":"BoostingDecisionTrees\n\nA Julia module for decision tree and boosting algorithms, including decision stumps, Gini impurity, and information gain utilities.\n\nOverview\n\nThis module provides tools for training and evaluating simple decision trees and decision stumps, with support for both Gini impurity and information gain as splitting criteria. It is designed for educational purposes and prototyping machine learning models.\n\nFeatures\n\nDecision Stumps: Simple binary classifiers based on a single feature threshold.\nSplitting Criteria: Supports both Gini impurity and information gain for feature selection.\nUtilities: Includes helper functions for entropy, Gini impurity, and majority voting.\n\nExports\n\nDecision Stump Functions:\nDecisionStump: A type representing a decision stump.\ntrain_stump: Train a decision stump on a dataset.\npredict_stump: Make predictions using a trained decision stump.\nGini Impurity:\nbest_split: Find the best threshold to split a feature vector using Gini impurity.\nInformation Gain:\nbest_split_information_gain: Find the feature index with the highest information gain.\n\nExamples\n\njulia> using BoostingDecisionTrees\n\njulia> X = [1.0 2.0; 3.0 0.5; 2.0 1.5];\n\njulia> y = [\"a\", \"b\", \"a\"];\n\njulia> stump = train_stump(X, y)\nDecisionStump(1, 2.5, \"a\", \"b\")\n\njulia> predict_stump(stump, X)\n3-element Vector{Any}:\n \"a\"\n \"b\"\n \"a\"\n\nNotes\n\nIntended as a compact educational toolkit and a foundation for experiments with boosting and small decision-tree learners.\n\n\n\n\n\n","category":"module"},{"location":"#BoostingDecisionTrees.DecisionStump","page":"Documentation","title":"BoostingDecisionTrees.DecisionStump","text":"DecisionStump\n\nA simple binary classifier based on a single feature threshold.\n\nFields\n\nfeature::Int: Index of the feature to split on.\nthreshold::Float64: Threshold value for the split.\nleft_label: Class label for samples where feature value <= threshold.\nright_label: Class label for samples where feature value > threshold.\n\nExamples\n\njulia> stump = DecisionStump(1, 2.5, \"A\", \"B\")\nDecisionStump(1, 2.5, \"A\", \"B\")\n\n\n\n\n\n","category":"type"},{"location":"#BoostingDecisionTrees.best_split-Tuple{Any, Any}","page":"Documentation","title":"BoostingDecisionTrees.best_split","text":"best_split(feature, labels)\n\nFind the best threshold to split a feature vector for minimizing Gini impurity.\n\nArguments\n\nfeature::AbstractVector{<:Real}: A vector of numerical feature values.\nlabels::AbstractVector: a vector of class labels (same length as feature)\n\nReturns\n\nbest_threshold::Union{Float64, Nothing}: The best numerical value to split the feature on.\n\nReturns nothing if no split is possible.\n\nbest_gini::Float64: The weighted Gini impurity after the split.\n\nExamples\n\njulia> feature = [1.0, 2.0, 3.0, 4.0];\n\njulia> labels = [\"A\", \"A\", \"B\", \"B\"];\n\njulia> best_threshold, best_gini = best_split(feature, labels)\n(2.5, 0.0)\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.best_split_information_gain-Tuple{Matrix, Vector}","page":"Documentation","title":"BoostingDecisionTrees.best_split_information_gain","text":"best_split_information_gain(X::Matrix, y::Vector)\n\nFind the feature index that yields the highest information gain.\n\nArguments\n\nX::Matrix: A matrix where each column is a feature and each row is a sample. y::Vector: A vector of class labels, with the same number of rows as X.\n\nReturns\n\nbest_feature::Int: The index of the feature with the highest information gain. best_gain::Float64: The highest information gain value found.\n\nExamples\n\njulia> X = [1 1; 1 2; 2 1; 2 2];\n\njulia> y = [\"a\", \"a\", \"b\", \"b\"];\n\njulia> best_split_information_gain(X, y)\n(1, 1.0)\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.entropy-Tuple{Vector}","page":"Documentation","title":"BoostingDecisionTrees.entropy","text":"entropy(y::Vector)\n\nCompute the entropy of a vector of class labels.\n\nArguments\n\ny::Vector: A vector of class labels.\n\nReturns\n\nFloat64: The entropy of the input vector.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.gini_impurity-Tuple{Any}","page":"Documentation","title":"BoostingDecisionTrees.gini_impurity","text":"gini_impurity(classes)\n\nCompute the Gini impurity of a vector of class labels.\n\nArguments\n\nclasses::AbstractVector: A collection of class labels.\n\nReturns\n\nFloat64: The Gini impurity of the input vector. Returns 0 if the input is empty.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.information_gain-Tuple{Vector, Vector}","page":"Documentation","title":"BoostingDecisionTrees.information_gain","text":"information_gain(X_column::Vector, y::Vector)\n\nCompute the information gain obtained by splitting on a feature column. This calculates how helpful a selector is by comparing entropy of a feature before and after applying it.\n\nArguments\n\nX_column::Vector: A vector of feature values.\ny::Vector: A vector of class labels, with the same length as X_column.\n\nReturns\n\nFloat64: The information gain from splitting on the feature column.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.majority_label-Tuple{Any}","page":"Documentation","title":"BoostingDecisionTrees.majority_label","text":"majority_label(y)\n\nReturn the most common class label in y.\n\nArguments\n\ny::AbstractVector: a collection of class labels.\n\nReturns\n\nThe label that appears most frequently in y. If there is a tie, one of the tied labels is returned.\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.predict_stump-Tuple{DecisionStump, AbstractMatrix}","page":"Documentation","title":"BoostingDecisionTrees.predict_stump","text":"predict_stump(stump, X)\n\nMake predictions using the trained decision stump.\n\nArguments\n\n'stump::DecisionStump': a trained decision stump model\n'X'::AbstractMatrix': rows are samples, columns are features\n\nReturns\n\nVector{Any}: predicted class labels for each sample in X\n\nExamples\n\njulia> stump = DecisionStump(1, 2.5, \"A\", \"B\");\n\njulia> X = [1.0 2.0; 3.0 0.5; 2.0 1.5];\n\njulia> preds = predict_stump(stump, X)\n3-element Vector{Any}:\n \"A\"\n \"B\"\n \"A\"\n\n\n\n\n\n","category":"method"},{"location":"#BoostingDecisionTrees.train_stump-Tuple{AbstractMatrix, AbstractVector}","page":"Documentation","title":"BoostingDecisionTrees.train_stump","text":"train_stump(X, y)\n\nTrain a decision stump classifier on the dataset.\n\nArguments\n\nX::AbstractMatrix: rows are samples, columns are features.\ny::AbstractVector: class labels for each sample.\n\nReturns\n\nDecisionStump: a trained stump with feature, threshold, left_label, and right_label.\n\nExamples\n\njulia> X = [1.0 2.0; 3.0 0.5; 2.0 1.5];\n\njulia> y = [\"a\", \"b\", \"a\"];\n\njulia> stump = train_stump(X, y)\nDecisionStump(1, 2.5, \"a\", \"b\")\n\n\n\n\n\n","category":"method"}]
}
